{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,obs_size, hidden_size,n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(obs_size,hidden_size),\n",
    "                                nn.Tanh(),\n",
    "                                nn.Linear(hidden_size,n_actions))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Categorical()"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Categorical(logits = torch.FloatTensor([1,0,1]))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41, 14, 45]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = [0,0,0]\n",
    "for i in range(0,100):\n",
    "    x = c.sample().item()\n",
    "    count[x]+=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.8620)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.log_prob(torch.as_tensor(0, dtype=torch.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2, 2, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x =([2]*8)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 2, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = ([1]*3 + [2]*5)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2500)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.FloatTensor(x)*torch.FloatTensor(y)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make function to compute action distribution\n",
    "def get_policy(obs, net):\n",
    "    logits = net(obs)\n",
    "    return Categorical(logits=logits)\n",
    "\n",
    "# make action selection function (outputs int actions, sampled from policy)\n",
    "def get_action(obs, net):\n",
    "    return get_policy(obs, net).sample().item()\n",
    "\n",
    "# make loss function whose gradient, for the right data, is policy gradient\n",
    "def compute_loss(obs, act, weights, net):\n",
    "    logp = get_policy(obs,net).log_prob(act)\n",
    "    return -(logp * weights).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training policy\n",
    "\n",
    "def train2(env, net, optimizer,epochs = 50, batch_size=5000, lr=1e-2, render=False):\n",
    "    \n",
    "#     optimizer = Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "#     # make function to compute action distribution\n",
    "#     def get_policy(obs):\n",
    "#         logits = net(obs)\n",
    "#         return Categorical(logits=logits)\n",
    "\n",
    "#     # make action selection function (outputs int actions, sampled from policy)\n",
    "#     def get_action(obs):\n",
    "#         return get_policy(obs).sample().item()\n",
    "\n",
    "#     # make loss function whose gradient, for the right data, is policy gradient\n",
    "#     def compute_loss(obs, act, weights):\n",
    "#         logp = get_policy(obs).log_prob(act)\n",
    "#         return -(logp * weights).mean()\n",
    "\n",
    "    def get_action(obs):\n",
    "#         obs_v = torch.as_tensor(obs,dtype=torch.float32)\n",
    "        logits = net(obs)\n",
    "        acts_sm = Categorical(logits=logits)\n",
    "        act = acts_sm.sample().item()\n",
    "        return act\n",
    "    \n",
    "    def compute_loss(obs, act, weights):\n",
    "    #     obs_v = torch.FloatTensor(obs)\n",
    "        policy = Categorical(logits = net(obs))\n",
    "        log_p = policy.log_prob(act)\n",
    "        return -(log_p*weights).mean()\n",
    "    \n",
    "    \n",
    "\n",
    "    def train_one_epoch():\n",
    "        batch_obs = []\n",
    "        batch_wts = []\n",
    "        batch_acts = []\n",
    "        batch_rets = []\n",
    "        batch_len = []\n",
    "        eps_rew = []\n",
    "        obs = env.reset()\n",
    "        done=False\n",
    "        epoch_finished_rendering = False\n",
    "\n",
    "        while True:\n",
    "            if not epoch_finished_rendering and render:\n",
    "                env.render()\n",
    "\n",
    "            act = get_action(torch.as_tensor(obs,dtype=torch.float32))\n",
    "            \n",
    "            batch_obs.append(obs.copy())\n",
    "            \n",
    "            batch_acts.append(act)\n",
    "\n",
    "            obs,rew,done,_ = env.step(act)\n",
    "\n",
    "            eps_rew.append(rew)\n",
    "\n",
    "    #         obs= next_obs\n",
    "\n",
    "            if done:\n",
    "                eps_ret = sum(eps_rew)\n",
    "                eps_len = len(eps_rew)\n",
    "                batch_rets.append(eps_ret)\n",
    "                batch_len.append(eps_len)\n",
    "\n",
    "                batch_wts = batch_wts + [eps_ret]*eps_len\n",
    "\n",
    "                eps_rew = []\n",
    "                done = False\n",
    "\n",
    "                obs = env.reset()\n",
    "                epoch_finished_rendering = True\n",
    "\n",
    "                if len(batch_obs)>batch_size:\n",
    "                    break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = compute_loss(obs = torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                                  act = torch.as_tensor(batch_acts, dtype = torch.int32),\n",
    "                                  weights = torch.as_tensor(batch_wts, dtype = torch.float32))\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        return batch_loss,batch_rets, batch_len\n",
    "    \n",
    "    # training loop\n",
    "    for i in range(epochs):\n",
    "        batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "                (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "obs_size, n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 32\n",
    "BATCH_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(obs_size = obs_size, hidden_size = HIDDEN_SIZE, n_actions= n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "lr = 1e-2\n",
    "optimizer = Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(comment=\"-vanilla_policy_grad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 \t loss: 21.896 \t return: 24.019 \t ep_len: 24.019\n",
      "epoch:   2 \t loss: 22.237 \t return: 24.169 \t ep_len: 24.169\n",
      "epoch:   3 \t loss: 21.804 \t return: 24.488 \t ep_len: 24.488\n",
      "epoch:   4 \t loss: 23.991 \t return: 25.738 \t ep_len: 25.738\n",
      "epoch:   5 \t loss: 20.532 \t return: 23.838 \t ep_len: 23.838\n",
      "epoch:   6 \t loss: 21.582 \t return: 24.184 \t ep_len: 24.184\n",
      "epoch:   7 \t loss: 22.240 \t return: 25.025 \t ep_len: 25.025\n",
      "epoch:   8 \t loss: 22.508 \t return: 24.549 \t ep_len: 24.549\n",
      "epoch:   9 \t loss: 21.976 \t return: 23.943 \t ep_len: 23.943\n",
      "epoch:  10 \t loss: 21.975 \t return: 24.072 \t ep_len: 24.072\n",
      "epoch:  11 \t loss: 22.045 \t return: 23.590 \t ep_len: 23.590\n",
      "epoch:  12 \t loss: 20.400 \t return: 23.245 \t ep_len: 23.245\n",
      "epoch:  13 \t loss: 23.419 \t return: 25.263 \t ep_len: 25.263\n",
      "epoch:  14 \t loss: 24.044 \t return: 26.681 \t ep_len: 26.681\n",
      "epoch:  15 \t loss: 23.757 \t return: 26.067 \t ep_len: 26.067\n",
      "epoch:  16 \t loss: 22.940 \t return: 25.582 \t ep_len: 25.582\n",
      "epoch:  17 \t loss: 19.582 \t return: 23.171 \t ep_len: 23.171\n",
      "epoch:  18 \t loss: 20.775 \t return: 23.938 \t ep_len: 23.938\n",
      "epoch:  19 \t loss: 20.993 \t return: 23.754 \t ep_len: 23.754\n",
      "epoch:  20 \t loss: 21.951 \t return: 24.493 \t ep_len: 24.493\n",
      "epoch:  21 \t loss: 22.206 \t return: 24.434 \t ep_len: 24.434\n",
      "epoch:  22 \t loss: 19.600 \t return: 23.372 \t ep_len: 23.372\n",
      "epoch:  23 \t loss: 21.234 \t return: 24.729 \t ep_len: 24.729\n",
      "epoch:  24 \t loss: 22.392 \t return: 25.258 \t ep_len: 25.258\n",
      "epoch:  25 \t loss: 22.309 \t return: 24.454 \t ep_len: 24.454\n",
      "epoch:  26 \t loss: 20.939 \t return: 24.330 \t ep_len: 24.330\n",
      "epoch:  27 \t loss: 22.429 \t return: 25.536 \t ep_len: 25.536\n",
      "epoch:  28 \t loss: 22.026 \t return: 24.925 \t ep_len: 24.925\n",
      "epoch:  29 \t loss: 22.486 \t return: 24.672 \t ep_len: 24.672\n",
      "epoch:  30 \t loss: 21.893 \t return: 24.659 \t ep_len: 24.659\n",
      "epoch:  31 \t loss: 21.991 \t return: 24.256 \t ep_len: 24.256\n",
      "epoch:  32 \t loss: 21.858 \t return: 25.327 \t ep_len: 25.327\n",
      "epoch:  33 \t loss: 22.047 \t return: 25.085 \t ep_len: 25.085\n",
      "epoch:  34 \t loss: 22.412 \t return: 23.703 \t ep_len: 23.703\n",
      "epoch:  35 \t loss: 22.588 \t return: 25.161 \t ep_len: 25.161\n",
      "epoch:  36 \t loss: 22.269 \t return: 24.802 \t ep_len: 24.802\n",
      "epoch:  37 \t loss: 22.218 \t return: 24.291 \t ep_len: 24.291\n",
      "epoch:  38 \t loss: 24.456 \t return: 25.487 \t ep_len: 25.487\n",
      "epoch:  39 \t loss: 23.743 \t return: 26.236 \t ep_len: 26.236\n",
      "epoch:  40 \t loss: 22.618 \t return: 25.835 \t ep_len: 25.835\n",
      "epoch:  41 \t loss: 20.704 \t return: 24.222 \t ep_len: 24.222\n",
      "epoch:  42 \t loss: 23.222 \t return: 26.005 \t ep_len: 26.005\n",
      "epoch:  43 \t loss: 23.557 \t return: 25.141 \t ep_len: 25.141\n",
      "epoch:  44 \t loss: 23.676 \t return: 26.219 \t ep_len: 26.219\n",
      "epoch:  45 \t loss: 25.376 \t return: 26.649 \t ep_len: 26.649\n",
      "epoch:  46 \t loss: 21.050 \t return: 23.952 \t ep_len: 23.952\n",
      "epoch:  47 \t loss: 21.276 \t return: 23.947 \t ep_len: 23.947\n",
      "epoch:  48 \t loss: 22.418 \t return: 24.905 \t ep_len: 24.905\n",
      "epoch:  49 \t loss: 19.760 \t return: 23.157 \t ep_len: 23.157\n",
      "epoch:  50 \t loss: 20.387 \t return: 23.241 \t ep_len: 23.241\n",
      "epoch:  51 \t loss: 20.668 \t return: 23.957 \t ep_len: 23.957\n",
      "epoch:  52 \t loss: 20.065 \t return: 23.479 \t ep_len: 23.479\n",
      "epoch:  53 \t loss: 21.660 \t return: 24.539 \t ep_len: 24.539\n",
      "epoch:  54 \t loss: 24.076 \t return: 25.313 \t ep_len: 25.313\n",
      "epoch:  55 \t loss: 24.328 \t return: 25.085 \t ep_len: 25.085\n",
      "epoch:  56 \t loss: 23.461 \t return: 25.607 \t ep_len: 25.607\n",
      "epoch:  57 \t loss: 20.881 \t return: 24.410 \t ep_len: 24.410\n",
      "epoch:  58 \t loss: 20.976 \t return: 24.359 \t ep_len: 24.359\n",
      "epoch:  59 \t loss: 20.918 \t return: 24.345 \t ep_len: 24.345\n",
      "epoch:  60 \t loss: 22.880 \t return: 25.442 \t ep_len: 25.442\n",
      "epoch:  61 \t loss: 21.375 \t return: 24.398 \t ep_len: 24.398\n",
      "epoch:  62 \t loss: 22.229 \t return: 25.146 \t ep_len: 25.146\n",
      "epoch:  63 \t loss: 21.108 \t return: 24.277 \t ep_len: 24.277\n",
      "epoch:  64 \t loss: 21.247 \t return: 24.184 \t ep_len: 24.184\n",
      "epoch:  65 \t loss: 23.783 \t return: 24.420 \t ep_len: 24.420\n",
      "epoch:  66 \t loss: 20.180 \t return: 23.689 \t ep_len: 23.689\n",
      "epoch:  67 \t loss: 23.021 \t return: 24.564 \t ep_len: 24.564\n",
      "epoch:  68 \t loss: 21.665 \t return: 24.217 \t ep_len: 24.217\n",
      "epoch:  69 \t loss: 23.090 \t return: 25.261 \t ep_len: 25.261\n",
      "epoch:  70 \t loss: 22.354 \t return: 24.106 \t ep_len: 24.106\n",
      "epoch:  71 \t loss: 21.736 \t return: 24.043 \t ep_len: 24.043\n",
      "epoch:  72 \t loss: 24.231 \t return: 26.047 \t ep_len: 26.047\n",
      "epoch:  73 \t loss: 23.269 \t return: 25.582 \t ep_len: 25.582\n",
      "epoch:  74 \t loss: 20.146 \t return: 22.854 \t ep_len: 22.854\n",
      "epoch:  75 \t loss: 21.128 \t return: 23.554 \t ep_len: 23.554\n",
      "epoch:  76 \t loss: 20.789 \t return: 24.208 \t ep_len: 24.208\n",
      "epoch:  77 \t loss: 21.315 \t return: 25.150 \t ep_len: 25.150\n",
      "epoch:  78 \t loss: 24.163 \t return: 25.718 \t ep_len: 25.718\n",
      "epoch:  79 \t loss: 21.150 \t return: 24.029 \t ep_len: 24.029\n",
      "epoch:  80 \t loss: 24.671 \t return: 25.396 \t ep_len: 25.396\n",
      "epoch:  81 \t loss: 21.246 \t return: 23.632 \t ep_len: 23.632\n",
      "epoch:  82 \t loss: 22.996 \t return: 25.010 \t ep_len: 25.010\n",
      "epoch:  83 \t loss: 21.741 \t return: 25.447 \t ep_len: 25.447\n",
      "epoch:  84 \t loss: 23.451 \t return: 24.266 \t ep_len: 24.266\n",
      "epoch:  85 \t loss: 19.925 \t return: 22.791 \t ep_len: 22.791\n",
      "epoch:  86 \t loss: 21.593 \t return: 24.369 \t ep_len: 24.369\n",
      "epoch:  87 \t loss: 21.358 \t return: 24.364 \t ep_len: 24.364\n",
      "epoch:  88 \t loss: 21.674 \t return: 24.554 \t ep_len: 24.554\n",
      "epoch:  89 \t loss: 21.284 \t return: 24.374 \t ep_len: 24.374\n",
      "epoch:  90 \t loss: 21.512 \t return: 23.709 \t ep_len: 23.709\n",
      "epoch:  91 \t loss: 22.955 \t return: 25.416 \t ep_len: 25.416\n",
      "epoch:  92 \t loss: 21.503 \t return: 25.352 \t ep_len: 25.352\n",
      "epoch:  93 \t loss: 21.114 \t return: 24.955 \t ep_len: 24.955\n",
      "epoch:  94 \t loss: 20.407 \t return: 23.928 \t ep_len: 23.928\n",
      "epoch:  95 \t loss: 23.831 \t return: 24.767 \t ep_len: 24.767\n",
      "epoch:  96 \t loss: 22.252 \t return: 24.593 \t ep_len: 24.593\n",
      "epoch:  97 \t loss: 21.753 \t return: 24.515 \t ep_len: 24.515\n",
      "epoch:  98 \t loss: 22.781 \t return: 25.692 \t ep_len: 25.692\n",
      "epoch:  99 \t loss: 23.131 \t return: 26.209 \t ep_len: 26.209\n",
      "epoch: 100 \t loss: 21.642 \t return: 24.588 \t ep_len: 24.588\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "rew_req = 200\n",
    "i=0\n",
    "mean_rew = 0\n",
    "while i<100:\n",
    "    i+=1\n",
    "#     render = True if i%100==0 else False\n",
    "    render = False\n",
    "    batch_loss,batch_ret, batch_len = train_one_epoch(env=env,batch_size=BATCH_SIZE, net=net, optimizer=optimizer,\n",
    "                                                      render=render)\n",
    "    mean_rew = np.mean(batch_ret)\n",
    "    if render:\n",
    "        env.close()\n",
    "    print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "                (i, batch_loss, np.mean(batch_ret), np.mean(batch_len)))\n",
    "    writer.add_scalar(\"loss\", batch_loss, i)\n",
    "    writer.add_scalar(\"reward_mean\", mean_rew, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 15.0,\n",
       " ...]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 \t loss: 19.815 \t return: 21.595 \t ep_len: 21.595\n",
      "epoch:   1 \t loss: 22.077 \t return: 24.198 \t ep_len: 24.198\n",
      "epoch:   2 \t loss: 27.823 \t return: 28.455 \t ep_len: 28.455\n",
      "epoch:   3 \t loss: 27.448 \t return: 29.427 \t ep_len: 29.427\n",
      "epoch:   4 \t loss: 30.861 \t return: 35.549 \t ep_len: 35.549\n",
      "epoch:   5 \t loss: 33.756 \t return: 37.358 \t ep_len: 37.358\n",
      "epoch:   6 \t loss: 33.728 \t return: 39.968 \t ep_len: 39.968\n",
      "epoch:   7 \t loss: 38.875 \t return: 44.105 \t ep_len: 44.105\n",
      "epoch:   8 \t loss: 41.882 \t return: 47.358 \t ep_len: 47.358\n",
      "epoch:   9 \t loss: 41.350 \t return: 50.160 \t ep_len: 50.160\n",
      "epoch:  10 \t loss: 40.570 \t return: 52.853 \t ep_len: 52.853\n",
      "epoch:  11 \t loss: 44.237 \t return: 54.978 \t ep_len: 54.978\n",
      "epoch:  12 \t loss: 42.222 \t return: 57.736 \t ep_len: 57.736\n",
      "epoch:  13 \t loss: 45.412 \t return: 61.753 \t ep_len: 61.753\n",
      "epoch:  14 \t loss: 45.116 \t return: 62.185 \t ep_len: 62.185\n",
      "epoch:  15 \t loss: 52.168 \t return: 71.789 \t ep_len: 71.789\n",
      "epoch:  16 \t loss: 49.939 \t return: 67.480 \t ep_len: 67.480\n",
      "epoch:  17 \t loss: 49.604 \t return: 72.300 \t ep_len: 72.300\n",
      "epoch:  18 \t loss: 45.960 \t return: 65.558 \t ep_len: 65.558\n",
      "epoch:  19 \t loss: 56.328 \t return: 80.823 \t ep_len: 80.823\n",
      "epoch:  20 \t loss: 55.110 \t return: 80.889 \t ep_len: 80.889\n",
      "epoch:  21 \t loss: 58.292 \t return: 82.508 \t ep_len: 82.508\n",
      "epoch:  22 \t loss: 55.902 \t return: 83.433 \t ep_len: 83.433\n",
      "epoch:  23 \t loss: 66.431 \t return: 96.577 \t ep_len: 96.577\n",
      "epoch:  24 \t loss: 65.808 \t return: 95.093 \t ep_len: 95.093\n",
      "epoch:  25 \t loss: 75.719 \t return: 112.667 \t ep_len: 112.667\n",
      "epoch:  26 \t loss: 86.454 \t return: 129.256 \t ep_len: 129.256\n",
      "epoch:  27 \t loss: 91.223 \t return: 140.444 \t ep_len: 140.444\n",
      "epoch:  28 \t loss: 105.950 \t return: 172.448 \t ep_len: 172.448\n",
      "epoch:  29 \t loss: 104.826 \t return: 166.581 \t ep_len: 166.581\n",
      "epoch:  30 \t loss: 104.666 \t return: 169.200 \t ep_len: 169.200\n",
      "epoch:  31 \t loss: 107.085 \t return: 173.828 \t ep_len: 173.828\n",
      "epoch:  32 \t loss: 110.719 \t return: 181.500 \t ep_len: 181.500\n",
      "epoch:  33 \t loss: 110.708 \t return: 180.607 \t ep_len: 180.607\n",
      "epoch:  34 \t loss: 109.508 \t return: 181.000 \t ep_len: 181.000\n",
      "epoch:  35 \t loss: 110.420 \t return: 179.429 \t ep_len: 179.429\n",
      "epoch:  36 \t loss: 112.679 \t return: 189.741 \t ep_len: 189.741\n",
      "epoch:  37 \t loss: 110.777 \t return: 189.407 \t ep_len: 189.407\n",
      "epoch:  38 \t loss: 111.589 \t return: 189.222 \t ep_len: 189.222\n",
      "epoch:  39 \t loss: 111.153 \t return: 187.630 \t ep_len: 187.630\n",
      "epoch:  40 \t loss: 113.520 \t return: 195.423 \t ep_len: 195.423\n",
      "epoch:  41 \t loss: 111.498 \t return: 191.889 \t ep_len: 191.889\n",
      "epoch:  42 \t loss: 113.818 \t return: 196.538 \t ep_len: 196.538\n",
      "epoch:  43 \t loss: 112.124 \t return: 192.692 \t ep_len: 192.692\n",
      "epoch:  44 \t loss: 112.050 \t return: 190.481 \t ep_len: 190.481\n",
      "epoch:  45 \t loss: 110.198 \t return: 190.926 \t ep_len: 190.926\n",
      "epoch:  46 \t loss: 114.487 \t return: 198.115 \t ep_len: 198.115\n",
      "epoch:  47 \t loss: 112.213 \t return: 196.346 \t ep_len: 196.346\n",
      "epoch:  48 \t loss: 114.971 \t return: 200.000 \t ep_len: 200.000\n",
      "epoch:  49 \t loss: 113.401 \t return: 193.115 \t ep_len: 193.115\n"
     ]
    }
   ],
   "source": [
    "train(env_name = 'CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 \t loss: 15.750 \t return: 18.913 \t ep_len: 18.913\n",
      "epoch:   1 \t loss: 18.182 \t return: 20.727 \t ep_len: 20.727\n",
      "epoch:   2 \t loss: 23.703 \t return: 25.528 \t ep_len: 25.528\n",
      "epoch:   3 \t loss: 25.573 \t return: 28.401 \t ep_len: 28.401\n",
      "epoch:   4 \t loss: 25.571 \t return: 28.426 \t ep_len: 28.426\n",
      "epoch:   5 \t loss: 28.797 \t return: 31.478 \t ep_len: 31.478\n",
      "epoch:   6 \t loss: 28.968 \t return: 34.517 \t ep_len: 34.517\n",
      "epoch:   7 \t loss: 35.402 \t return: 38.569 \t ep_len: 38.569\n",
      "epoch:   8 \t loss: 38.308 \t return: 46.315 \t ep_len: 46.315\n",
      "epoch:   9 \t loss: 41.127 \t return: 47.990 \t ep_len: 47.990\n",
      "epoch:  10 \t loss: 46.987 \t return: 55.656 \t ep_len: 55.656\n",
      "epoch:  11 \t loss: 39.756 \t return: 53.042 \t ep_len: 53.042\n",
      "epoch:  12 \t loss: 52.824 \t return: 63.062 \t ep_len: 63.062\n",
      "epoch:  13 \t loss: 50.006 \t return: 66.145 \t ep_len: 66.145\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-342-c9f7014e49c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-337-7494e32bffe2>\u001b[0m in \u001b[0;36mtrain2\u001b[1;34m(env, net, optimizer, epochs, batch_size, lr, render)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;31m# training loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mbatch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_rets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_lens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n\u001b[0;32m     92\u001b[0m                 (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n",
      "\u001b[1;32m<ipython-input-337-7494e32bffe2>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mbatch_obs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-337-7494e32bffe2>\u001b[0m in \u001b[0;36mget_action\u001b[1;34m(obs)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#         obs_v = torch.as_tensor(obs,dtype=torch.float32)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0macts_sm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macts_sm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\distributions\\categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_events\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train2(env=env, net=net, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.3292841e-01,  1.3936392e+38,  2.2844117e-02,  8.5078421e+37],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.observation_space.sample()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "def train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2, \n",
    "          epochs=50, batch_size=5000, render=False):\n",
    "\n",
    "    # make environment, check spaces, get obs / act dims\n",
    "    env = gym.make(env_name)\n",
    "    assert isinstance(env.observation_space, Box), \\\n",
    "        \"This example only works for envs with continuous state spaces.\"\n",
    "    assert isinstance(env.action_space, Discrete), \\\n",
    "        \"This example only works for envs with discrete action spaces.\"\n",
    "\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_acts = env.action_space.n\n",
    "\n",
    "    # make core of policy network\n",
    "    logits_net = Net(obs_dim, hidden_sizes[0], n_acts)\n",
    "\n",
    "    # make function to compute action distribution\n",
    "    def get_policy(obs):\n",
    "        logits = logits_net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    # make action selection function (outputs int actions, sampled from policy)\n",
    "    def get_action(obs):\n",
    "        return get_policy(obs).sample().item()\n",
    "\n",
    "    # make loss function whose gradient, for the right data, is policy gradient\n",
    "    def compute_loss(obs, act, weights):\n",
    "        logp = get_policy(obs).log_prob(act)\n",
    "        return -(logp * weights).mean()\n",
    "\n",
    "    # make optimizer\n",
    "    optimizer = Adam(logits_net.parameters(), lr=lr)\n",
    "\n",
    "    # for training policy\n",
    "    def train_one_epoch():\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []          # for observations\n",
    "        batch_acts = []         # for actions\n",
    "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "        batch_rets = []         # for measuring episode returns\n",
    "        batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        obs = env.reset()       # first obs comes from starting distribution\n",
    "        done = False            # signal from environment that episode is over\n",
    "        ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "        # render first episode of each epoch\n",
    "        finished_rendering_this_epoch = False\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        while True:\n",
    "\n",
    "            # rendering\n",
    "            if (not finished_rendering_this_epoch) and render:\n",
    "                env.render()\n",
    "\n",
    "            # save obs\n",
    "            batch_obs.append(obs.copy())\n",
    "\n",
    "            # act in the environment\n",
    "            act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            obs, rew, done, _ = env.step(act)\n",
    "\n",
    "            # save action, reward\n",
    "            batch_acts.append(act)\n",
    "            ep_rews.append(rew)\n",
    "\n",
    "            if done:\n",
    "                # if episode is over, record info about episode\n",
    "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "                batch_rets.append(ep_ret)\n",
    "                batch_lens.append(ep_len)\n",
    "\n",
    "                # the weight for each logprob(a|s) is R(tau)\n",
    "                batch_weights += [ep_ret] * ep_len\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                obs, done, ep_rews = env.reset(), False, []\n",
    "\n",
    "                # won't render again this epoch\n",
    "                finished_rendering_this_epoch = True\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_obs) > batch_size:\n",
    "                    break\n",
    "\n",
    "        # take a single policy gradient update step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                                  act=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
    "                                  weights=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
    "                                  )\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        return batch_loss, batch_rets, batch_lens\n",
    "\n",
    "    # training loop\n",
    "    for i in range(epochs):\n",
    "        batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "                (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([79045296309277814133001641918405279744.,\n",
       "         8074543887068950992878338433657864192.], grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.FloatTensor(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2 = Categorical(net(torch.FloatTensor(obs)))\n",
    "c2.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.3786, grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2.log_prob(torch.as_tensor(1, dtype = torch.int32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
