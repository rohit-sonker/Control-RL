{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 100\n",
    "GAMMA = 0.9\n",
    "PERCENTILE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net,self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                nn.Linear(obs_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation','action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs=env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "#         if len(batch)==batch_size-1:\n",
    "#             env.render()\n",
    "        \n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        \n",
    "        action = np.random.choice(len(act_probs), p = act_probs)\n",
    "        next_obs, rew, is_done, _ = env.step(action)\n",
    "        \n",
    "        episode_reward+=rew\n",
    "        episode_steps.append(EpisodeStep(observation = obs, action = action))\n",
    "        \n",
    "        if is_done:\n",
    "            \n",
    "#             print(episode_reward)\n",
    "            \n",
    "            batch.append(Episode(reward = episode_reward, steps = episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps=[]\n",
    "            next_obs = env.reset()\n",
    "            if len(batch)==batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "             \n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    disc_rewards = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), batch))\n",
    "    reward_bound = np.percentile(disc_rewards, percentile)\n",
    "#     reward_mean = float(np.mean(rewards))\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_batch=[]\n",
    "    for example, discounted_reward in zip(batch, disc_rewards):\n",
    "        if discounted_reward>reward_bound:\n",
    "            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "            train_act.extend(map(lambda step: step.action, example.steps))\n",
    "            elite_batch.append(example)\n",
    "    \n",
    "#     train_obs_v = torch.FloatTensor(train_obs)\n",
    "#     train_act_v = torch.LongTensor(train_act)\n",
    "#     return train_obs_v, train_act_v, reward_bound, reward_mean\n",
    "    return elite_batch, train_obs, train_act, reward_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "#explore env\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# main run\n",
    "random.seed(12345)\n",
    "# # env = gym.make(\"FrozenLake-v0\")\n",
    "# env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "\n",
    "#non slippery version\n",
    "env = gym.envs.toy_text.frozen_lake.FrozenLakeEnv(is_slippery=False)\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=100)\n",
    "env = DiscreteOneHotWrapper(env)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params = net.parameters(), lr=0.001)\n",
    "writer = SummaryWriter(comment=\"-frozenlake-nonslip-tweaked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.273, reward_mean=0.0, reward_bound=0.0 ,batch=2\n",
      "1: loss=1.298, reward_mean=0.1, reward_bound=0.0 ,batch=7\n",
      "2: loss=1.305, reward_mean=0.0, reward_bound=0.0 ,batch=11\n",
      "3: loss=1.301, reward_mean=0.0, reward_bound=0.0 ,batch=11\n",
      "4: loss=1.296, reward_mean=0.0, reward_bound=0.0 ,batch=13\n",
      "5: loss=1.275, reward_mean=0.1, reward_bound=0.0 ,batch=20\n",
      "6: loss=1.282, reward_mean=0.0, reward_bound=0.0 ,batch=24\n",
      "7: loss=1.275, reward_mean=0.1, reward_bound=0.0 ,batch=31\n",
      "8: loss=1.266, reward_mean=0.1, reward_bound=0.0 ,batch=38\n",
      "9: loss=1.261, reward_mean=0.0, reward_bound=0.0 ,batch=41\n",
      "10: loss=1.260, reward_mean=0.1, reward_bound=0.0 ,batch=49\n",
      "11: loss=1.255, reward_mean=0.0, reward_bound=0.0 ,batch=53\n",
      "12: loss=1.247, reward_mean=0.0, reward_bound=0.0 ,batch=57\n",
      "13: loss=1.239, reward_mean=0.1, reward_bound=0.0 ,batch=62\n",
      "14: loss=1.234, reward_mean=0.0, reward_bound=0.0 ,batch=64\n",
      "15: loss=1.225, reward_mean=0.0, reward_bound=0.0 ,batch=68\n",
      "16: loss=1.218, reward_mean=0.1, reward_bound=0.0 ,batch=74\n",
      "17: loss=1.211, reward_mean=0.0, reward_bound=0.0 ,batch=78\n",
      "18: loss=1.207, reward_mean=0.1, reward_bound=0.0 ,batch=83\n",
      "19: loss=1.206, reward_mean=0.1, reward_bound=0.0 ,batch=91\n",
      "20: loss=1.198, reward_mean=0.1, reward_bound=0.0 ,batch=103\n",
      "21: loss=1.195, reward_mean=0.0, reward_bound=0.0 ,batch=107\n",
      "22: loss=1.190, reward_mean=0.1, reward_bound=0.0 ,batch=114\n",
      "23: loss=1.185, reward_mean=0.1, reward_bound=0.0 ,batch=122\n",
      "24: loss=1.181, reward_mean=0.1, reward_bound=0.0 ,batch=132\n",
      "25: loss=1.177, reward_mean=0.1, reward_bound=0.0 ,batch=142\n",
      "26: loss=1.171, reward_mean=0.1, reward_bound=0.0 ,batch=153\n",
      "27: loss=1.170, reward_mean=0.1, reward_bound=0.0 ,batch=164\n",
      "28: loss=1.163, reward_mean=0.1, reward_bound=0.0 ,batch=172\n",
      "29: loss=1.159, reward_mean=0.1, reward_bound=0.0 ,batch=187\n",
      "30: loss=1.154, reward_mean=0.1, reward_bound=0.0 ,batch=201\n",
      "31: loss=1.139, reward_mean=0.1, reward_bound=0.1 ,batch=207\n",
      "32: loss=1.128, reward_mean=0.1, reward_bound=0.1 ,batch=213\n",
      "33: loss=1.112, reward_mean=0.1, reward_bound=0.2 ,batch=217\n",
      "34: loss=1.096, reward_mean=0.1, reward_bound=0.2 ,batch=218\n",
      "35: loss=1.072, reward_mean=0.2, reward_bound=0.2 ,batch=212\n",
      "36: loss=1.053, reward_mean=0.1, reward_bound=0.3 ,batch=214\n",
      "37: loss=1.026, reward_mean=0.2, reward_bound=0.3 ,batch=206\n",
      "38: loss=1.022, reward_mean=0.2, reward_bound=0.3 ,batch=213\n",
      "39: loss=0.992, reward_mean=0.2, reward_bound=0.3 ,batch=198\n",
      "40: loss=0.950, reward_mean=0.2, reward_bound=0.3 ,batch=167\n",
      "41: loss=0.954, reward_mean=0.2, reward_bound=0.2 ,batch=186\n",
      "42: loss=0.940, reward_mean=0.2, reward_bound=0.3 ,batch=199\n",
      "43: loss=0.934, reward_mean=0.2, reward_bound=0.3 ,batch=209\n",
      "44: loss=0.918, reward_mean=0.2, reward_bound=0.3 ,batch=210\n",
      "45: loss=0.871, reward_mean=0.2, reward_bound=0.4 ,batch=170\n",
      "46: loss=0.871, reward_mean=0.2, reward_bound=0.3 ,batch=189\n",
      "47: loss=0.859, reward_mean=0.2, reward_bound=0.3 ,batch=202\n",
      "48: loss=0.841, reward_mean=0.3, reward_bound=0.4 ,batch=210\n",
      "49: loss=0.777, reward_mean=0.3, reward_bound=0.4 ,batch=143\n",
      "50: loss=0.795, reward_mean=0.3, reward_bound=0.0 ,batch=169\n",
      "51: loss=0.785, reward_mean=0.2, reward_bound=0.3 ,batch=187\n",
      "52: loss=0.754, reward_mean=0.3, reward_bound=0.4 ,batch=188\n",
      "53: loss=0.724, reward_mean=0.3, reward_bound=0.4 ,batch=186\n",
      "54: loss=0.642, reward_mean=0.3, reward_bound=0.5 ,batch=96\n",
      "55: loss=0.753, reward_mean=0.3, reward_bound=0.0 ,batch=130\n",
      "56: loss=0.750, reward_mean=0.4, reward_bound=0.2 ,batch=160\n",
      "57: loss=0.684, reward_mean=0.4, reward_bound=0.3 ,batch=174\n",
      "58: loss=0.653, reward_mean=0.3, reward_bound=0.4 ,batch=184\n",
      "59: loss=0.613, reward_mean=0.3, reward_bound=0.4 ,batch=174\n",
      "60: loss=0.570, reward_mean=0.4, reward_bound=0.5 ,batch=147\n",
      "61: loss=0.593, reward_mean=0.3, reward_bound=0.4 ,batch=173\n",
      "62: loss=0.580, reward_mean=0.3, reward_bound=0.4 ,batch=189\n",
      "63: loss=0.547, reward_mean=0.4, reward_bound=0.4 ,batch=200\n",
      "64: loss=0.523, reward_mean=0.4, reward_bound=0.5 ,batch=190\n",
      "66: loss=0.745, reward_mean=0.3, reward_bound=0.0 ,batch=35\n",
      "67: loss=0.760, reward_mean=0.4, reward_bound=0.0 ,batch=75\n",
      "68: loss=0.751, reward_mean=0.4, reward_bound=0.0 ,batch=119\n",
      "69: loss=0.677, reward_mean=0.4, reward_bound=0.3 ,batch=147\n",
      "70: loss=0.611, reward_mean=0.4, reward_bound=0.4 ,batch=154\n",
      "71: loss=0.545, reward_mean=0.6, reward_bound=0.4 ,batch=150\n",
      "72: loss=0.467, reward_mean=0.5, reward_bound=0.5 ,batch=112\n",
      "73: loss=0.505, reward_mean=0.5, reward_bound=0.4 ,batch=148\n",
      "74: loss=0.475, reward_mean=0.5, reward_bound=0.4 ,batch=166\n",
      "75: loss=0.439, reward_mean=0.5, reward_bound=0.5 ,batch=166\n",
      "77: loss=0.609, reward_mean=0.5, reward_bound=0.0 ,batch=53\n",
      "78: loss=0.570, reward_mean=0.6, reward_bound=0.4 ,batch=100\n",
      "79: loss=0.510, reward_mean=0.5, reward_bound=0.4 ,batch=112\n",
      "80: loss=0.400, reward_mean=0.6, reward_bound=0.5 ,batch=96\n",
      "81: loss=0.387, reward_mean=0.6, reward_bound=0.5 ,batch=128\n",
      "82: loss=0.381, reward_mean=0.6, reward_bound=0.5 ,batch=159\n",
      "84: loss=0.528, reward_mean=0.5, reward_bound=0.0 ,batch=53\n",
      "85: loss=0.551, reward_mean=0.5, reward_bound=0.0 ,batch=104\n",
      "86: loss=0.471, reward_mean=0.6, reward_bound=0.4 ,batch=140\n",
      "87: loss=0.420, reward_mean=0.6, reward_bound=0.4 ,batch=159\n",
      "88: loss=0.422, reward_mean=0.5, reward_bound=0.5 ,batch=202\n",
      "89: loss=0.349, reward_mean=0.7, reward_bound=0.5 ,batch=175\n",
      "91: loss=0.525, reward_mean=0.7, reward_bound=0.0 ,batch=68\n",
      "92: loss=0.410, reward_mean=0.7, reward_bound=0.4 ,batch=108\n",
      "93: loss=0.397, reward_mean=0.6, reward_bound=0.5 ,batch=156\n",
      "94: loss=0.318, reward_mean=0.6, reward_bound=0.5 ,batch=140\n",
      "96: loss=0.534, reward_mean=0.7, reward_bound=0.0 ,batch=68\n",
      "97: loss=0.453, reward_mean=0.6, reward_bound=0.4 ,batch=116\n",
      "98: loss=0.315, reward_mean=0.8, reward_bound=0.5 ,batch=104\n",
      "99: loss=0.307, reward_mean=0.7, reward_bound=0.5 ,batch=141\n",
      "101: loss=0.419, reward_mean=0.7, reward_bound=0.4 ,batch=66\n",
      "102: loss=0.308, reward_mean=0.7, reward_bound=0.5 ,batch=68\n",
      "103: loss=0.296, reward_mean=0.8, reward_bound=0.5 ,batch=114\n",
      "105: loss=0.502, reward_mean=0.6, reward_bound=0.0 ,batch=62\n",
      "106: loss=0.268, reward_mean=0.8, reward_bound=0.5 ,batch=87\n",
      "107: loss=0.277, reward_mean=0.7, reward_bound=0.5 ,batch=127\n",
      "109: loss=0.356, reward_mean=0.7, reward_bound=0.4 ,batch=70\n",
      "110: loss=0.268, reward_mean=0.8, reward_bound=0.5 ,batch=96\n",
      "112: loss=0.353, reward_mean=0.8, reward_bound=0.4 ,batch=67\n",
      "113: loss=0.260, reward_mean=0.8, reward_bound=0.5 ,batch=99\n",
      "115: loss=0.331, reward_mean=0.8, reward_bound=0.4 ,batch=68\n",
      "116: loss=0.250, reward_mean=0.8, reward_bound=0.5 ,batch=105\n",
      "118: loss=0.347, reward_mean=0.8, reward_bound=0.5 ,batch=73\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "#loop through\n",
    "\n",
    "full_batch = []\n",
    "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    reward_m = float(np.mean(list(map(lambda s: s.reward, batch))))\n",
    "    full_batch, obs, acts, reward_b = filter_batch(full_batch+batch, PERCENTILE)\n",
    "    if not full_batch:\n",
    "        continue\n",
    "    obs_v = torch.FloatTensor(obs)\n",
    "    act_v = torch.LongTensor(acts)\n",
    "    \n",
    "    full_batch = full_batch[-500:]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = objective(action_scores_v, act_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f ,batch=%d\"% (\n",
    "        iter_no, loss_v.item(), reward_m, reward_b,len(full_batch)))\n",
    "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "    \n",
    "    if reward_m > 0.8:\n",
    "        print(\"Solved!\")\n",
    "        break\n",
    "        \n",
    "writer.close()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(obs, net):\n",
    "    \n",
    "    obs_v = torch.FloatTensor([obs])\n",
    "    act_probs_v = sm(net(obs_v))\n",
    "    act_probs = act_probs_v.data.numpy()[0]\n",
    "\n",
    "    action = np.random.choice(len(act_probs), p = act_probs)\n",
    "    return action\n",
    "\n",
    "net_trained = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0  Total Reward =  1.0  Total Steps =  6\n",
      "iter 1  Total Reward =  1.0  Total Steps =  6\n",
      "iter 2  Total Reward =  1.0  Total Steps =  6\n",
      "iter 3  Total Reward =  1.0  Total Steps =  6\n",
      "iter 4  Total Reward =  1.0  Total Steps =  8\n",
      "iter 5  Total Reward =  1.0  Total Steps =  6\n",
      "iter 6  Total Reward =  1.0  Total Steps =  6\n",
      "iter 7  Total Reward =  0.0  Total Steps =  5\n",
      "iter 8  Total Reward =  1.0  Total Steps =  6\n",
      "iter 9  Total Reward =  1.0  Total Steps =  6\n",
      "iter 10  Total Reward =  1.0  Total Steps =  6\n",
      "iter 11  Total Reward =  1.0  Total Steps =  6\n",
      "iter 12  Total Reward =  1.0  Total Steps =  12\n",
      "iter 13  Total Reward =  0.0  Total Steps =  2\n",
      "iter 14  Total Reward =  1.0  Total Steps =  9\n",
      "iter 15  Total Reward =  0.0  Total Steps =  3\n",
      "iter 16  Total Reward =  1.0  Total Steps =  6\n",
      "iter 17  Total Reward =  1.0  Total Steps =  8\n",
      "iter 18  Total Reward =  1.0  Total Steps =  6\n",
      "iter 19  Total Reward =  1.0  Total Steps =  6\n",
      "iter 20  Total Reward =  1.0  Total Steps =  6\n",
      "iter 21  Total Reward =  0.0  Total Steps =  5\n",
      "iter 22  Total Reward =  1.0  Total Steps =  6\n",
      "iter 23  Total Reward =  1.0  Total Steps =  6\n",
      "iter 24  Total Reward =  1.0  Total Steps =  6\n",
      "iter 25  Total Reward =  1.0  Total Steps =  6\n",
      "iter 26  Total Reward =  1.0  Total Steps =  6\n",
      "iter 27  Total Reward =  1.0  Total Steps =  8\n",
      "iter 28  Total Reward =  1.0  Total Steps =  6\n",
      "iter 29  Total Reward =  1.0  Total Steps =  6\n",
      "iter 30  Total Reward =  1.0  Total Steps =  6\n",
      "iter 31  Total Reward =  1.0  Total Steps =  7\n",
      "iter 32  Total Reward =  1.0  Total Steps =  6\n",
      "iter 33  Total Reward =  1.0  Total Steps =  6\n",
      "iter 34  Total Reward =  1.0  Total Steps =  6\n",
      "iter 35  Total Reward =  0.0  Total Steps =  3\n",
      "iter 36  Total Reward =  1.0  Total Steps =  7\n",
      "iter 37  Total Reward =  1.0  Total Steps =  6\n",
      "iter 38  Total Reward =  1.0  Total Steps =  6\n",
      "iter 39  Total Reward =  1.0  Total Steps =  7\n",
      "iter 40  Total Reward =  1.0  Total Steps =  6\n",
      "iter 41  Total Reward =  1.0  Total Steps =  7\n",
      "iter 42  Total Reward =  0.0  Total Steps =  3\n",
      "iter 43  Total Reward =  1.0  Total Steps =  6\n",
      "iter 44  Total Reward =  1.0  Total Steps =  6\n",
      "iter 45  Total Reward =  1.0  Total Steps =  6\n",
      "iter 46  Total Reward =  0.0  Total Steps =  5\n",
      "iter 47  Total Reward =  1.0  Total Steps =  6\n",
      "iter 48  Total Reward =  1.0  Total Steps =  7\n",
      "iter 49  Total Reward =  1.0  Total Steps =  7\n",
      "iter 50  Total Reward =  1.0  Total Steps =  6\n",
      "iter 51  Total Reward =  1.0  Total Steps =  6\n",
      "iter 52  Total Reward =  1.0  Total Steps =  6\n",
      "iter 53  Total Reward =  0.0  Total Steps =  3\n",
      "iter 54  Total Reward =  0.0  Total Steps =  3\n",
      "iter 55  Total Reward =  1.0  Total Steps =  6\n",
      "iter 56  Total Reward =  1.0  Total Steps =  6\n",
      "iter 57  Total Reward =  0.0  Total Steps =  4\n",
      "iter 58  Total Reward =  0.0  Total Steps =  2\n",
      "iter 59  Total Reward =  1.0  Total Steps =  6\n",
      "iter 60  Total Reward =  1.0  Total Steps =  6\n",
      "iter 61  Total Reward =  1.0  Total Steps =  8\n",
      "iter 62  Total Reward =  1.0  Total Steps =  7\n",
      "iter 63  Total Reward =  1.0  Total Steps =  6\n",
      "iter 64  Total Reward =  1.0  Total Steps =  6\n",
      "iter 65  Total Reward =  0.0  Total Steps =  4\n",
      "iter 66  Total Reward =  1.0  Total Steps =  6\n",
      "iter 67  Total Reward =  1.0  Total Steps =  6\n",
      "iter 68  Total Reward =  1.0  Total Steps =  6\n",
      "iter 69  Total Reward =  1.0  Total Steps =  6\n",
      "iter 70  Total Reward =  1.0  Total Steps =  6\n",
      "iter 71  Total Reward =  0.0  Total Steps =  5\n",
      "iter 72  Total Reward =  0.0  Total Steps =  2\n",
      "iter 73  Total Reward =  1.0  Total Steps =  6\n",
      "iter 74  Total Reward =  1.0  Total Steps =  8\n",
      "iter 75  Total Reward =  1.0  Total Steps =  7\n",
      "iter 76  Total Reward =  1.0  Total Steps =  6\n",
      "iter 77  Total Reward =  1.0  Total Steps =  6\n",
      "iter 78  Total Reward =  0.0  Total Steps =  2\n",
      "iter 79  Total Reward =  0.0  Total Steps =  4\n",
      "iter 80  Total Reward =  1.0  Total Steps =  7\n",
      "iter 81  Total Reward =  0.0  Total Steps =  4\n",
      "iter 82  Total Reward =  1.0  Total Steps =  6\n",
      "iter 83  Total Reward =  1.0  Total Steps =  7\n",
      "iter 84  Total Reward =  0.0  Total Steps =  3\n",
      "iter 85  Total Reward =  0.0  Total Steps =  7\n",
      "iter 86  Total Reward =  1.0  Total Steps =  6\n",
      "iter 87  Total Reward =  1.0  Total Steps =  6\n",
      "iter 88  Total Reward =  1.0  Total Steps =  6\n",
      "iter 89  Total Reward =  0.0  Total Steps =  2\n",
      "iter 90  Total Reward =  0.0  Total Steps =  2\n",
      "iter 91  Total Reward =  0.0  Total Steps =  3\n",
      "iter 92  Total Reward =  1.0  Total Steps =  6\n",
      "iter 93  Total Reward =  0.0  Total Steps =  2\n",
      "iter 94  Total Reward =  1.0  Total Steps =  6\n",
      "iter 95  Total Reward =  1.0  Total Steps =  7\n",
      "iter 96  Total Reward =  1.0  Total Steps =  7\n",
      "iter 97  Total Reward =  1.0  Total Steps =  7\n",
      "iter 98  Total Reward =  1.0  Total Steps =  6\n",
      "iter 99  Total Reward =  1.0  Total Steps =  7\n",
      "iter 100  Total Reward =  1.0  Total Steps =  8\n",
      "iter 101  Total Reward =  1.0  Total Steps =  7\n",
      "iter 102  Total Reward =  1.0  Total Steps =  7\n",
      "iter 103  Total Reward =  1.0  Total Steps =  9\n",
      "iter 104  Total Reward =  1.0  Total Steps =  7\n",
      "iter 105  Total Reward =  1.0  Total Steps =  7\n",
      "iter 106  Total Reward =  0.0  Total Steps =  3\n",
      "iter 107  Total Reward =  0.0  Total Steps =  2\n",
      "iter 108  Total Reward =  1.0  Total Steps =  6\n",
      "iter 109  Total Reward =  1.0  Total Steps =  7\n",
      "iter 110  Total Reward =  0.0  Total Steps =  3\n",
      "iter 111  Total Reward =  1.0  Total Steps =  6\n",
      "iter 112  Total Reward =  1.0  Total Steps =  8\n",
      "iter 113  Total Reward =  0.0  Total Steps =  5\n",
      "iter 114  Total Reward =  1.0  Total Steps =  8\n",
      "iter 115  Total Reward =  1.0  Total Steps =  7\n",
      "iter 116  Total Reward =  1.0  Total Steps =  6\n",
      "iter 117  Total Reward =  1.0  Total Steps =  6\n",
      "iter 118  Total Reward =  1.0  Total Steps =  6\n",
      "iter 119  Total Reward =  0.0  Total Steps =  3\n",
      "iter 120  Total Reward =  1.0  Total Steps =  6\n",
      "iter 121  Total Reward =  1.0  Total Steps =  7\n",
      "iter 122  Total Reward =  1.0  Total Steps =  8\n",
      "iter 123  Total Reward =  0.0  Total Steps =  2\n",
      "iter 124  Total Reward =  1.0  Total Steps =  6\n",
      "iter 125  Total Reward =  1.0  Total Steps =  6\n",
      "iter 126  Total Reward =  1.0  Total Steps =  6\n",
      "iter 127  Total Reward =  1.0  Total Steps =  7\n",
      "iter 128  Total Reward =  1.0  Total Steps =  6\n",
      "iter 129  Total Reward =  1.0  Total Steps =  6\n",
      "iter 130  Total Reward =  1.0  Total Steps =  8\n",
      "iter 131  Total Reward =  1.0  Total Steps =  6\n",
      "iter 132  Total Reward =  1.0  Total Steps =  13\n",
      "iter 133  Total Reward =  1.0  Total Steps =  6\n",
      "iter 134  Total Reward =  1.0  Total Steps =  6\n",
      "iter 135  Total Reward =  1.0  Total Steps =  6\n",
      "iter 136  Total Reward =  1.0  Total Steps =  6\n",
      "iter 137  Total Reward =  1.0  Total Steps =  6\n",
      "iter 138  Total Reward =  1.0  Total Steps =  8\n",
      "iter 139  Total Reward =  1.0  Total Steps =  6\n",
      "iter 140  Total Reward =  1.0  Total Steps =  6\n",
      "iter 141  Total Reward =  1.0  Total Steps =  7\n",
      "iter 142  Total Reward =  1.0  Total Steps =  8\n",
      "iter 143  Total Reward =  1.0  Total Steps =  7\n",
      "iter 144  Total Reward =  1.0  Total Steps =  6\n",
      "iter 145  Total Reward =  1.0  Total Steps =  6\n",
      "iter 146  Total Reward =  1.0  Total Steps =  7\n",
      "iter 147  Total Reward =  1.0  Total Steps =  6\n",
      "iter 148  Total Reward =  1.0  Total Steps =  7\n",
      "iter 149  Total Reward =  1.0  Total Steps =  7\n",
      "iter 150  Total Reward =  0.0  Total Steps =  2\n",
      "iter 151  Total Reward =  1.0  Total Steps =  6\n",
      "iter 152  Total Reward =  1.0  Total Steps =  6\n",
      "iter 153  Total Reward =  0.0  Total Steps =  3\n",
      "iter 154  Total Reward =  1.0  Total Steps =  6\n",
      "iter 155  Total Reward =  1.0  Total Steps =  6\n",
      "iter 156  Total Reward =  1.0  Total Steps =  6\n",
      "iter 157  Total Reward =  0.0  Total Steps =  2\n",
      "iter 158  Total Reward =  1.0  Total Steps =  6\n",
      "iter 159  Total Reward =  0.0  Total Steps =  2\n",
      "iter 160  Total Reward =  1.0  Total Steps =  7\n",
      "iter 161  Total Reward =  1.0  Total Steps =  9\n",
      "iter 162  Total Reward =  0.0  Total Steps =  2\n",
      "iter 163  Total Reward =  1.0  Total Steps =  8\n",
      "iter 164  Total Reward =  1.0  Total Steps =  6\n",
      "iter 165  Total Reward =  1.0  Total Steps =  6\n",
      "iter 166  Total Reward =  1.0  Total Steps =  6\n",
      "iter 167  Total Reward =  1.0  Total Steps =  6\n",
      "iter 168  Total Reward =  1.0  Total Steps =  6\n",
      "iter 169  Total Reward =  1.0  Total Steps =  6\n",
      "iter 170  Total Reward =  0.0  Total Steps =  4\n",
      "iter 171  Total Reward =  0.0  Total Steps =  4\n",
      "iter 172  Total Reward =  1.0  Total Steps =  6\n",
      "iter 173  Total Reward =  0.0  Total Steps =  2\n",
      "iter 174  Total Reward =  1.0  Total Steps =  6\n",
      "iter 175  Total Reward =  0.0  Total Steps =  3\n",
      "iter 176  Total Reward =  1.0  Total Steps =  6\n",
      "iter 177  Total Reward =  1.0  Total Steps =  6\n",
      "iter 178  Total Reward =  0.0  Total Steps =  5\n",
      "iter 179  Total Reward =  1.0  Total Steps =  6\n",
      "iter 180  Total Reward =  1.0  Total Steps =  6\n",
      "iter 181  Total Reward =  1.0  Total Steps =  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 182  Total Reward =  1.0  Total Steps =  6\n",
      "iter 183  Total Reward =  1.0  Total Steps =  8\n",
      "iter 184  Total Reward =  1.0  Total Steps =  6\n",
      "iter 185  Total Reward =  1.0  Total Steps =  6\n",
      "iter 186  Total Reward =  1.0  Total Steps =  8\n",
      "iter 187  Total Reward =  1.0  Total Steps =  8\n",
      "iter 188  Total Reward =  0.0  Total Steps =  3\n",
      "iter 189  Total Reward =  0.0  Total Steps =  4\n",
      "iter 190  Total Reward =  1.0  Total Steps =  6\n",
      "iter 191  Total Reward =  1.0  Total Steps =  8\n",
      "iter 192  Total Reward =  0.0  Total Steps =  2\n",
      "iter 193  Total Reward =  1.0  Total Steps =  6\n",
      "iter 194  Total Reward =  1.0  Total Steps =  6\n",
      "iter 195  Total Reward =  1.0  Total Steps =  6\n",
      "iter 196  Total Reward =  0.0  Total Steps =  3\n",
      "iter 197  Total Reward =  1.0  Total Steps =  6\n",
      "iter 198  Total Reward =  0.0  Total Steps =  5\n",
      "iter 199  Total Reward =  1.0  Total Steps =  6\n",
      "avg rew =  1.56\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# playing with trained agent\n",
    "total_reward_a = []\n",
    "total_steps = 0\n",
    "# obs = env.reset()\n",
    "sm = nn.Softmax(dim=1)\n",
    "\n",
    "for i in range(200):\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "#         env.render()\n",
    "        action = get_action(obs, net_trained)\n",
    "#         action = env.action_space.sample()\n",
    "        next_obs, rew, is_done, _ = env.step(action)\n",
    "        total_reward+=rew\n",
    "        total_steps+=1\n",
    "        if is_done:\n",
    "            print(\"iter\",i,\" Total Reward = \",total_reward,\" Total Steps = \", total_steps)            \n",
    "            total_reward_a.append(total_reward)\n",
    "            break\n",
    "        obs=next_obs\n",
    "\n",
    "print('avg rew = ',sum(total_reward_a)/BATCH_SIZE)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
