{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,obs_size, hidden_size,n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(obs_size,hidden_size),\n",
    "                                nn.Tanh(),\n",
    "                                nn.Linear(hidden_size,n_actions),\n",
    "                                nn.Tanh())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.net(x)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.normal import Normal\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(net, obs):\n",
    "    mu = net(obs)\n",
    "    return Normal(loc=mu, scale = STD)\n",
    "\n",
    "def get_action(net, obs):\n",
    "    policy = get_policy(net, obs)\n",
    "    act = policy.sample().item()\n",
    "#     if act<0:\n",
    "#         print (\"yes negative\")\n",
    "    return np.array([act])\n",
    "\n",
    "def reward_to_go(rews):\n",
    "    n = len(rews)\n",
    "    rtgs = np.zeros_like(rews)\n",
    "    for i in reversed(range(n)):\n",
    "        rtgs[i] = rews[i] + (rtgs[i+1] if i+1 < n else 0)\n",
    "    return rtgs\n",
    "\n",
    "def reward_to_go_avg(rews, avg):\n",
    "    n = len(rews)\n",
    "    rtgs = np.zeros_like(rews)\n",
    "    for i in reversed(range(n)):\n",
    "        rtgs[i] = rews[i] + (rtgs[i+1] if i+1 < n else 0) - avg/n\n",
    "    return rtgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(obs, acts, wts, net):\n",
    "#     obs_v = torch.FloatTensor(obs)\n",
    "    policy = get_policy(net, obs)\n",
    "    log_p = policy.log_prob(acts)\n",
    "    return -(log_p*wts).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(env, net, lr=1e-2, batch_size=5000, render=False):\n",
    "    batch_obs = []\n",
    "    batch_wts = []\n",
    "    batch_acts = []\n",
    "    batch_rets = []\n",
    "    batch_len = []\n",
    "    eps_rew = []\n",
    "    obs = env.reset()\n",
    "    done=False\n",
    "    epoch_finished_rendering = False\n",
    "    \n",
    "    while True:\n",
    "        if not epoch_finished_rendering and render:\n",
    "            env.render()\n",
    "        \n",
    "        act = get_action(net = net, obs=torch.as_tensor(obs, dtype=torch.float32))\n",
    "        batch_obs.append(obs.copy())\n",
    "        batch_acts.append(act)\n",
    "        \n",
    "        obs,rew,done,_ = env.step(act)\n",
    "        \n",
    "        eps_rew.append(rew)\n",
    "        \n",
    "#         obs= next_obs\n",
    "        \n",
    "        if done:\n",
    "            eps_ret = sum(eps_rew)\n",
    "            eps_len = len(eps_rew)\n",
    "            batch_rets.append(eps_ret)\n",
    "            batch_len.append(eps_len)\n",
    "            \n",
    "#             batch_wts = batch_wts + [eps_ret-avg_rew]*eps_len\n",
    "            \n",
    "            #rtg\n",
    "            \n",
    "            batch_wts = batch_wts + list(reward_to_go(eps_rew))\n",
    "    \n",
    "            eps_rew = []\n",
    "            done = False\n",
    "            \n",
    "            obs = env.reset()\n",
    "            epoch_finished_rendering = True\n",
    "            \n",
    "            if len(batch_obs)>batch_size:\n",
    "                break\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch_loss = compute_loss(obs = torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                              acts = torch.as_tensor(batch_acts, dtype = torch.float32),\n",
    "                              wts = torch.as_tensor(batch_wts, dtype = torch.float32),\n",
    "                             net = net)\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    return batch_loss,batch_rets, batch_len, batch_obs, batch_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rsonker001\\AppData\\Local\\Continuum\\anaconda3\\envs\\gym\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "env.reset()\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.shape[0]\n",
    "\n",
    "obs_size, n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(3,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(1,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 32\n",
    "BATCH_SIZE = 500\n",
    "STD = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(obs_size = obs_size, hidden_size = HIDDEN_SIZE, n_actions= n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8791483 , -0.52392477, -0.528469  ], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.observation_space.sample()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66151482])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act = get_action(net, obs=torch.as_tensor(obs, dtype=torch.float32))\n",
    "act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4070, 1.154033899307251, -0.9292287230491638, 0.21791781996460632)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing net for giving -ve actions\n",
    "count = 0\n",
    "acts = []\n",
    "for i in range(0,10000):\n",
    "    obs = env.observation_space.sample()\n",
    "    act = get_action(net, obs=torch.as_tensor(obs, dtype=torch.float32))\n",
    "    acts.append(act[0])\n",
    "    if act[0]<0:\n",
    "        count+=1\n",
    "count, max(acts),min(acts), np.mean(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.8395157], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = env.action_space.sample()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.97085151],\n",
       "        [ 0.23968176],\n",
       "        [ 0.3240764 ]]),\n",
       " array([-8.31567152]),\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(np.array([act]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "lr = 1e-2\n",
    "optimizer = Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(comment=\"-vanilla_policy_grad_pendulum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 \t loss: 794.376 \t return: -1591.808 \t ep_len: 200.000\n",
      "epoch:   2 \t loss: 791.970 \t return: -1815.895 \t ep_len: 200.000\n",
      "epoch:   3 \t loss: 822.486 \t return: -1596.419 \t ep_len: 200.000\n",
      "epoch:   4 \t loss: 830.146 \t return: -1775.162 \t ep_len: 200.000\n",
      "epoch:   5 \t loss: 772.236 \t return: -1688.951 \t ep_len: 200.000\n",
      "epoch:   6 \t loss: 781.073 \t return: -1700.107 \t ep_len: 200.000\n",
      "epoch:   7 \t loss: 809.624 \t return: -1730.106 \t ep_len: 200.000\n",
      "epoch:   8 \t loss: 807.312 \t return: -1703.212 \t ep_len: 200.000\n",
      "epoch:   9 \t loss: 773.556 \t return: -1647.466 \t ep_len: 200.000\n",
      "epoch:  10 \t loss: 755.512 \t return: -1581.642 \t ep_len: 200.000\n",
      "epoch:  11 \t loss: 736.659 \t return: -1596.090 \t ep_len: 200.000\n",
      "epoch:  12 \t loss: 747.811 \t return: -1554.769 \t ep_len: 200.000\n",
      "epoch:  13 \t loss: 741.158 \t return: -1622.474 \t ep_len: 200.000\n",
      "epoch:  14 \t loss: 722.535 \t return: -1574.599 \t ep_len: 200.000\n",
      "epoch:  15 \t loss: 743.212 \t return: -1513.065 \t ep_len: 200.000\n",
      "epoch:  16 \t loss: 755.680 \t return: -1682.267 \t ep_len: 200.000\n",
      "epoch:  17 \t loss: 723.257 \t return: -1445.933 \t ep_len: 200.000\n",
      "epoch:  18 \t loss: 661.390 \t return: -1255.530 \t ep_len: 200.000\n",
      "epoch:  19 \t loss: 787.304 \t return: -1721.993 \t ep_len: 200.000\n",
      "epoch:  20 \t loss: 746.833 \t return: -1582.465 \t ep_len: 200.000\n",
      "epoch:  21 \t loss: 765.595 \t return: -1588.035 \t ep_len: 200.000\n",
      "epoch:  22 \t loss: 755.757 \t return: -1719.529 \t ep_len: 200.000\n",
      "epoch:  23 \t loss: 822.155 \t return: -1702.740 \t ep_len: 200.000\n",
      "epoch:  24 \t loss: 804.815 \t return: -1632.211 \t ep_len: 200.000\n",
      "epoch:  25 \t loss: 793.949 \t return: -1746.281 \t ep_len: 200.000\n",
      "epoch:  26 \t loss: 751.525 \t return: -1591.675 \t ep_len: 200.000\n",
      "epoch:  27 \t loss: 821.294 \t return: -1755.000 \t ep_len: 200.000\n",
      "epoch:  28 \t loss: 792.643 \t return: -1712.688 \t ep_len: 200.000\n",
      "epoch:  29 \t loss: 769.321 \t return: -1566.690 \t ep_len: 200.000\n",
      "epoch:  30 \t loss: 781.955 \t return: -1637.344 \t ep_len: 200.000\n",
      "epoch:  31 \t loss: 815.548 \t return: -1738.702 \t ep_len: 200.000\n",
      "epoch:  32 \t loss: 818.885 \t return: -1838.009 \t ep_len: 200.000\n",
      "epoch:  33 \t loss: 781.670 \t return: -1697.540 \t ep_len: 200.000\n",
      "epoch:  34 \t loss: 824.986 \t return: -1742.829 \t ep_len: 200.000\n",
      "epoch:  35 \t loss: 855.073 \t return: -1812.920 \t ep_len: 200.000\n",
      "epoch:  36 \t loss: 805.584 \t return: -1818.007 \t ep_len: 200.000\n",
      "epoch:  37 \t loss: 849.687 \t return: -1739.516 \t ep_len: 200.000\n",
      "epoch:  38 \t loss: 884.728 \t return: -1784.188 \t ep_len: 200.000\n",
      "epoch:  39 \t loss: 850.041 \t return: -1872.037 \t ep_len: 200.000\n",
      "epoch:  40 \t loss: 829.237 \t return: -1808.782 \t ep_len: 200.000\n",
      "epoch:  41 \t loss: 771.134 \t return: -1567.831 \t ep_len: 200.000\n",
      "epoch:  42 \t loss: 675.321 \t return: -1438.049 \t ep_len: 200.000\n",
      "epoch:  43 \t loss: 557.631 \t return: -1140.449 \t ep_len: 200.000\n",
      "epoch:  44 \t loss: 564.326 \t return: -1279.577 \t ep_len: 200.000\n",
      "epoch:  45 \t loss: 495.976 \t return: -1033.929 \t ep_len: 200.000\n",
      "epoch:  46 \t loss: 510.834 \t return: -1082.136 \t ep_len: 200.000\n",
      "epoch:  47 \t loss: 584.861 \t return: -1239.386 \t ep_len: 200.000\n",
      "epoch:  48 \t loss: 559.354 \t return: -1217.903 \t ep_len: 200.000\n",
      "epoch:  49 \t loss: 581.681 \t return: -1404.339 \t ep_len: 200.000\n",
      "epoch:  50 \t loss: 586.822 \t return: -1270.573 \t ep_len: 200.000\n",
      "epoch:  51 \t loss: 623.582 \t return: -1370.949 \t ep_len: 200.000\n",
      "epoch:  52 \t loss: 646.713 \t return: -1422.616 \t ep_len: 200.000\n",
      "epoch:  53 \t loss: 667.745 \t return: -1422.217 \t ep_len: 200.000\n",
      "epoch:  54 \t loss: 666.493 \t return: -1490.586 \t ep_len: 200.000\n",
      "epoch:  55 \t loss: 658.662 \t return: -1512.072 \t ep_len: 200.000\n",
      "epoch:  56 \t loss: 696.056 \t return: -1575.240 \t ep_len: 200.000\n",
      "epoch:  57 \t loss: 726.287 \t return: -1582.038 \t ep_len: 200.000\n",
      "epoch:  58 \t loss: 744.218 \t return: -1563.323 \t ep_len: 200.000\n",
      "epoch:  59 \t loss: 703.160 \t return: -1596.249 \t ep_len: 200.000\n",
      "epoch:  60 \t loss: 710.905 \t return: -1577.127 \t ep_len: 200.000\n",
      "epoch:  61 \t loss: 760.590 \t return: -1607.003 \t ep_len: 200.000\n",
      "epoch:  62 \t loss: 769.180 \t return: -1604.733 \t ep_len: 200.000\n",
      "epoch:  63 \t loss: 760.278 \t return: -1587.329 \t ep_len: 200.000\n",
      "epoch:  64 \t loss: 718.998 \t return: -1602.021 \t ep_len: 200.000\n",
      "epoch:  65 \t loss: 710.021 \t return: -1608.991 \t ep_len: 200.000\n",
      "epoch:  66 \t loss: 741.149 \t return: -1610.769 \t ep_len: 200.000\n",
      "epoch:  67 \t loss: 746.144 \t return: -1620.929 \t ep_len: 200.000\n",
      "epoch:  68 \t loss: 699.747 \t return: -1624.489 \t ep_len: 200.000\n",
      "epoch:  69 \t loss: 770.142 \t return: -1638.938 \t ep_len: 200.000\n",
      "epoch:  70 \t loss: 717.891 \t return: -1608.030 \t ep_len: 200.000\n",
      "epoch:  71 \t loss: 708.891 \t return: -1585.816 \t ep_len: 200.000\n",
      "epoch:  72 \t loss: 743.400 \t return: -1603.420 \t ep_len: 200.000\n",
      "epoch:  73 \t loss: 705.161 \t return: -1576.919 \t ep_len: 200.000\n",
      "epoch:  74 \t loss: 740.165 \t return: -1626.747 \t ep_len: 200.000\n",
      "epoch:  75 \t loss: 754.988 \t return: -1628.271 \t ep_len: 200.000\n",
      "epoch:  76 \t loss: 718.190 \t return: -1598.927 \t ep_len: 200.000\n",
      "epoch:  77 \t loss: 679.077 \t return: -1545.776 \t ep_len: 200.000\n",
      "epoch:  78 \t loss: 687.038 \t return: -1558.532 \t ep_len: 200.000\n",
      "epoch:  79 \t loss: 749.467 \t return: -1603.221 \t ep_len: 200.000\n",
      "epoch:  80 \t loss: 746.852 \t return: -1597.207 \t ep_len: 200.000\n",
      "epoch:  81 \t loss: 771.028 \t return: -1636.980 \t ep_len: 200.000\n",
      "epoch:  82 \t loss: 752.844 \t return: -1642.406 \t ep_len: 200.000\n",
      "epoch:  83 \t loss: 760.793 \t return: -1639.522 \t ep_len: 200.000\n",
      "epoch:  84 \t loss: 702.514 \t return: -1579.356 \t ep_len: 200.000\n",
      "epoch:  85 \t loss: 748.901 \t return: -1644.360 \t ep_len: 200.000\n",
      "epoch:  86 \t loss: 737.538 \t return: -1581.460 \t ep_len: 200.000\n",
      "epoch:  87 \t loss: 718.224 \t return: -1605.740 \t ep_len: 200.000\n",
      "epoch:  88 \t loss: 709.288 \t return: -1602.819 \t ep_len: 200.000\n",
      "epoch:  89 \t loss: 741.557 \t return: -1617.179 \t ep_len: 200.000\n",
      "epoch:  90 \t loss: 765.686 \t return: -1615.539 \t ep_len: 200.000\n",
      "epoch:  91 \t loss: 740.325 \t return: -1565.745 \t ep_len: 200.000\n",
      "epoch:  92 \t loss: 757.394 \t return: -1616.868 \t ep_len: 200.000\n",
      "epoch:  93 \t loss: 685.237 \t return: -1616.441 \t ep_len: 200.000\n",
      "epoch:  94 \t loss: 792.747 \t return: -1594.646 \t ep_len: 200.000\n",
      "epoch:  95 \t loss: 687.364 \t return: -1568.364 \t ep_len: 200.000\n",
      "epoch:  96 \t loss: 751.771 \t return: -1632.508 \t ep_len: 200.000\n",
      "epoch:  97 \t loss: 740.098 \t return: -1619.206 \t ep_len: 200.000\n",
      "epoch:  98 \t loss: 761.180 \t return: -1598.805 \t ep_len: 200.000\n",
      "epoch:  99 \t loss: 734.935 \t return: -1617.490 \t ep_len: 200.000\n",
      "epoch: 100 \t loss: 744.027 \t return: -1634.416 \t ep_len: 200.000\n",
      "epoch: 101 \t loss: 780.990 \t return: -1618.286 \t ep_len: 200.000\n",
      "epoch: 102 \t loss: 741.694 \t return: -1603.507 \t ep_len: 200.000\n",
      "epoch: 103 \t loss: 766.080 \t return: -1628.822 \t ep_len: 200.000\n",
      "epoch: 104 \t loss: 756.034 \t return: -1565.434 \t ep_len: 200.000\n",
      "epoch: 105 \t loss: 716.034 \t return: -1583.095 \t ep_len: 200.000\n",
      "epoch: 106 \t loss: 712.675 \t return: -1602.530 \t ep_len: 200.000\n",
      "epoch: 107 \t loss: 747.637 \t return: -1568.688 \t ep_len: 200.000\n",
      "epoch: 108 \t loss: 720.353 \t return: -1609.995 \t ep_len: 200.000\n",
      "epoch: 109 \t loss: 739.771 \t return: -1611.079 \t ep_len: 200.000\n",
      "epoch: 110 \t loss: 788.712 \t return: -1622.825 \t ep_len: 200.000\n",
      "epoch: 111 \t loss: 710.405 \t return: -1574.601 \t ep_len: 200.000\n",
      "epoch: 112 \t loss: 717.355 \t return: -1599.351 \t ep_len: 200.000\n",
      "epoch: 113 \t loss: 745.191 \t return: -1606.826 \t ep_len: 200.000\n",
      "epoch: 114 \t loss: 754.993 \t return: -1572.816 \t ep_len: 200.000\n",
      "epoch: 115 \t loss: 739.158 \t return: -1592.068 \t ep_len: 200.000\n",
      "epoch: 116 \t loss: 708.455 \t return: -1615.116 \t ep_len: 200.000\n",
      "epoch: 117 \t loss: 741.858 \t return: -1604.638 \t ep_len: 200.000\n",
      "epoch: 118 \t loss: 715.161 \t return: -1580.706 \t ep_len: 200.000\n",
      "epoch: 119 \t loss: 768.101 \t return: -1632.408 \t ep_len: 200.000\n",
      "epoch: 120 \t loss: 732.986 \t return: -1613.192 \t ep_len: 200.000\n",
      "epoch: 121 \t loss: 728.497 \t return: -1616.955 \t ep_len: 200.000\n",
      "epoch: 122 \t loss: 789.165 \t return: -1642.586 \t ep_len: 200.000\n",
      "epoch: 123 \t loss: 725.106 \t return: -1604.363 \t ep_len: 200.000\n",
      "epoch: 124 \t loss: 747.157 \t return: -1611.290 \t ep_len: 200.000\n",
      "epoch: 125 \t loss: 730.998 \t return: -1576.003 \t ep_len: 200.000\n",
      "epoch: 126 \t loss: 786.904 \t return: -1597.919 \t ep_len: 200.000\n",
      "epoch: 127 \t loss: 745.250 \t return: -1644.547 \t ep_len: 200.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 128 \t loss: 775.789 \t return: -1622.908 \t ep_len: 200.000\n",
      "epoch: 129 \t loss: 715.902 \t return: -1596.653 \t ep_len: 200.000\n",
      "epoch: 130 \t loss: 751.007 \t return: -1647.389 \t ep_len: 200.000\n",
      "epoch: 131 \t loss: 750.694 \t return: -1621.438 \t ep_len: 200.000\n",
      "epoch: 132 \t loss: 780.184 \t return: -1585.266 \t ep_len: 200.000\n",
      "epoch: 133 \t loss: 786.993 \t return: -1614.500 \t ep_len: 200.000\n",
      "epoch: 134 \t loss: 722.932 \t return: -1556.709 \t ep_len: 200.000\n",
      "epoch: 135 \t loss: 724.506 \t return: -1613.115 \t ep_len: 200.000\n",
      "epoch: 136 \t loss: 707.734 \t return: -1600.408 \t ep_len: 200.000\n",
      "epoch: 137 \t loss: 735.292 \t return: -1581.160 \t ep_len: 200.000\n",
      "epoch: 138 \t loss: 739.357 \t return: -1585.667 \t ep_len: 200.000\n",
      "epoch: 139 \t loss: 757.552 \t return: -1607.242 \t ep_len: 200.000\n",
      "epoch: 140 \t loss: 735.749 \t return: -1633.336 \t ep_len: 200.000\n",
      "epoch: 141 \t loss: 776.574 \t return: -1625.076 \t ep_len: 200.000\n",
      "epoch: 142 \t loss: 745.590 \t return: -1630.139 \t ep_len: 200.000\n",
      "epoch: 143 \t loss: 756.162 \t return: -1624.253 \t ep_len: 200.000\n",
      "epoch: 144 \t loss: 746.974 \t return: -1580.575 \t ep_len: 200.000\n",
      "epoch: 145 \t loss: 751.426 \t return: -1625.032 \t ep_len: 200.000\n",
      "epoch: 146 \t loss: 762.724 \t return: -1618.862 \t ep_len: 200.000\n",
      "epoch: 147 \t loss: 760.791 \t return: -1618.181 \t ep_len: 200.000\n",
      "epoch: 148 \t loss: 756.153 \t return: -1576.722 \t ep_len: 200.000\n",
      "epoch: 149 \t loss: 726.615 \t return: -1602.382 \t ep_len: 200.000\n",
      "epoch: 150 \t loss: 737.158 \t return: -1622.763 \t ep_len: 200.000\n",
      "epoch: 151 \t loss: 757.057 \t return: -1634.535 \t ep_len: 200.000\n",
      "epoch: 152 \t loss: 737.808 \t return: -1618.157 \t ep_len: 200.000\n",
      "epoch: 153 \t loss: 747.196 \t return: -1594.928 \t ep_len: 200.000\n",
      "epoch: 154 \t loss: 746.739 \t return: -1628.046 \t ep_len: 200.000\n",
      "epoch: 155 \t loss: 712.672 \t return: -1602.561 \t ep_len: 200.000\n",
      "epoch: 156 \t loss: 731.415 \t return: -1605.118 \t ep_len: 200.000\n",
      "epoch: 157 \t loss: 752.948 \t return: -1573.731 \t ep_len: 200.000\n",
      "epoch: 158 \t loss: 724.862 \t return: -1537.055 \t ep_len: 200.000\n",
      "epoch: 159 \t loss: 714.840 \t return: -1585.882 \t ep_len: 200.000\n",
      "epoch: 160 \t loss: 711.054 \t return: -1569.099 \t ep_len: 200.000\n",
      "epoch: 161 \t loss: 787.734 \t return: -1618.154 \t ep_len: 200.000\n",
      "epoch: 162 \t loss: 738.394 \t return: -1573.424 \t ep_len: 200.000\n",
      "epoch: 163 \t loss: 695.364 \t return: -1629.326 \t ep_len: 200.000\n",
      "epoch: 164 \t loss: 724.555 \t return: -1601.417 \t ep_len: 200.000\n",
      "epoch: 165 \t loss: 766.193 \t return: -1584.360 \t ep_len: 200.000\n",
      "epoch: 166 \t loss: 695.897 \t return: -1558.036 \t ep_len: 200.000\n",
      "epoch: 167 \t loss: 785.461 \t return: -1637.139 \t ep_len: 200.000\n",
      "epoch: 168 \t loss: 805.280 \t return: -1641.264 \t ep_len: 200.000\n",
      "epoch: 169 \t loss: 705.955 \t return: -1592.052 \t ep_len: 200.000\n",
      "epoch: 170 \t loss: 740.662 \t return: -1624.050 \t ep_len: 200.000\n",
      "epoch: 171 \t loss: 741.383 \t return: -1542.869 \t ep_len: 200.000\n",
      "epoch: 172 \t loss: 721.113 \t return: -1569.399 \t ep_len: 200.000\n",
      "epoch: 173 \t loss: 714.316 \t return: -1626.626 \t ep_len: 200.000\n",
      "epoch: 174 \t loss: 686.639 \t return: -1577.523 \t ep_len: 200.000\n",
      "epoch: 175 \t loss: 748.337 \t return: -1622.703 \t ep_len: 200.000\n",
      "epoch: 176 \t loss: 768.039 \t return: -1622.810 \t ep_len: 200.000\n",
      "epoch: 177 \t loss: 750.496 \t return: -1628.248 \t ep_len: 200.000\n",
      "epoch: 178 \t loss: 781.553 \t return: -1606.887 \t ep_len: 200.000\n",
      "epoch: 179 \t loss: 720.723 \t return: -1610.358 \t ep_len: 200.000\n",
      "epoch: 180 \t loss: 746.366 \t return: -1620.425 \t ep_len: 200.000\n",
      "epoch: 181 \t loss: 754.920 \t return: -1627.065 \t ep_len: 200.000\n",
      "epoch: 182 \t loss: 725.108 \t return: -1616.799 \t ep_len: 200.000\n",
      "epoch: 183 \t loss: 776.417 \t return: -1618.782 \t ep_len: 200.000\n",
      "epoch: 184 \t loss: 745.693 \t return: -1610.534 \t ep_len: 200.000\n",
      "epoch: 185 \t loss: 719.550 \t return: -1574.356 \t ep_len: 200.000\n",
      "epoch: 186 \t loss: 748.628 \t return: -1630.455 \t ep_len: 200.000\n",
      "epoch: 187 \t loss: 708.260 \t return: -1593.809 \t ep_len: 200.000\n",
      "epoch: 188 \t loss: 757.792 \t return: -1618.584 \t ep_len: 200.000\n",
      "epoch: 189 \t loss: 755.831 \t return: -1603.806 \t ep_len: 200.000\n",
      "epoch: 190 \t loss: 715.934 \t return: -1564.664 \t ep_len: 200.000\n",
      "epoch: 191 \t loss: 702.279 \t return: -1581.121 \t ep_len: 200.000\n",
      "epoch: 192 \t loss: 683.710 \t return: -1523.375 \t ep_len: 200.000\n",
      "epoch: 193 \t loss: 707.216 \t return: -1608.449 \t ep_len: 200.000\n",
      "epoch: 194 \t loss: 746.084 \t return: -1635.793 \t ep_len: 200.000\n",
      "epoch: 195 \t loss: 734.148 \t return: -1613.930 \t ep_len: 200.000\n",
      "epoch: 196 \t loss: 772.443 \t return: -1630.581 \t ep_len: 200.000\n",
      "epoch: 197 \t loss: 735.569 \t return: -1589.550 \t ep_len: 200.000\n",
      "epoch: 198 \t loss: 732.162 \t return: -1615.939 \t ep_len: 200.000\n",
      "epoch: 199 \t loss: 755.369 \t return: -1571.697 \t ep_len: 200.000\n",
      "epoch: 200 \t loss: 759.735 \t return: -1550.544 \t ep_len: 200.000\n",
      "epoch: 201 \t loss: 731.304 \t return: -1582.163 \t ep_len: 200.000\n",
      "epoch: 202 \t loss: 717.583 \t return: -1605.131 \t ep_len: 200.000\n",
      "epoch: 203 \t loss: 720.842 \t return: -1565.583 \t ep_len: 200.000\n",
      "epoch: 204 \t loss: 750.778 \t return: -1630.415 \t ep_len: 200.000\n",
      "epoch: 205 \t loss: 710.552 \t return: -1588.127 \t ep_len: 200.000\n",
      "epoch: 206 \t loss: 732.847 \t return: -1586.705 \t ep_len: 200.000\n",
      "epoch: 207 \t loss: 732.048 \t return: -1582.083 \t ep_len: 200.000\n",
      "epoch: 208 \t loss: 735.891 \t return: -1598.605 \t ep_len: 200.000\n",
      "epoch: 209 \t loss: 708.643 \t return: -1629.328 \t ep_len: 200.000\n",
      "epoch: 210 \t loss: 724.737 \t return: -1604.947 \t ep_len: 200.000\n",
      "epoch: 211 \t loss: 750.275 \t return: -1596.326 \t ep_len: 200.000\n",
      "epoch: 212 \t loss: 767.941 \t return: -1624.373 \t ep_len: 200.000\n",
      "epoch: 213 \t loss: 735.693 \t return: -1614.643 \t ep_len: 200.000\n",
      "epoch: 214 \t loss: 743.876 \t return: -1616.960 \t ep_len: 200.000\n",
      "epoch: 215 \t loss: 714.051 \t return: -1636.749 \t ep_len: 200.000\n",
      "epoch: 216 \t loss: 694.276 \t return: -1596.578 \t ep_len: 200.000\n",
      "epoch: 217 \t loss: 725.348 \t return: -1562.444 \t ep_len: 200.000\n",
      "epoch: 218 \t loss: 750.783 \t return: -1623.176 \t ep_len: 200.000\n",
      "epoch: 219 \t loss: 740.183 \t return: -1610.333 \t ep_len: 200.000\n",
      "epoch: 220 \t loss: 764.922 \t return: -1619.475 \t ep_len: 200.000\n",
      "epoch: 221 \t loss: 755.605 \t return: -1612.045 \t ep_len: 200.000\n",
      "epoch: 222 \t loss: 743.337 \t return: -1629.630 \t ep_len: 200.000\n",
      "epoch: 223 \t loss: 723.009 \t return: -1577.753 \t ep_len: 200.000\n",
      "epoch: 224 \t loss: 768.525 \t return: -1601.331 \t ep_len: 200.000\n",
      "epoch: 225 \t loss: 701.078 \t return: -1604.347 \t ep_len: 200.000\n",
      "epoch: 226 \t loss: 695.706 \t return: -1509.171 \t ep_len: 200.000\n",
      "epoch: 227 \t loss: 742.878 \t return: -1581.536 \t ep_len: 200.000\n",
      "epoch: 228 \t loss: 713.175 \t return: -1630.911 \t ep_len: 200.000\n",
      "epoch: 229 \t loss: 675.349 \t return: -1579.192 \t ep_len: 200.000\n",
      "epoch: 230 \t loss: 726.958 \t return: -1606.179 \t ep_len: 200.000\n",
      "epoch: 231 \t loss: 707.738 \t return: -1632.880 \t ep_len: 200.000\n",
      "epoch: 232 \t loss: 730.852 \t return: -1584.563 \t ep_len: 200.000\n",
      "epoch: 233 \t loss: 725.883 \t return: -1604.263 \t ep_len: 200.000\n",
      "epoch: 234 \t loss: 717.104 \t return: -1592.822 \t ep_len: 200.000\n",
      "epoch: 235 \t loss: 727.743 \t return: -1614.421 \t ep_len: 200.000\n",
      "epoch: 236 \t loss: 740.712 \t return: -1583.879 \t ep_len: 200.000\n",
      "epoch: 237 \t loss: 745.493 \t return: -1646.230 \t ep_len: 200.000\n",
      "epoch: 238 \t loss: 765.718 \t return: -1613.568 \t ep_len: 200.000\n",
      "epoch: 239 \t loss: 710.582 \t return: -1583.771 \t ep_len: 200.000\n",
      "epoch: 240 \t loss: 746.316 \t return: -1640.785 \t ep_len: 200.000\n",
      "epoch: 241 \t loss: 726.067 \t return: -1579.532 \t ep_len: 200.000\n",
      "epoch: 242 \t loss: 763.968 \t return: -1602.290 \t ep_len: 200.000\n",
      "epoch: 243 \t loss: 724.337 \t return: -1580.405 \t ep_len: 200.000\n",
      "epoch: 244 \t loss: 795.121 \t return: -1596.788 \t ep_len: 200.000\n",
      "epoch: 245 \t loss: 720.015 \t return: -1565.916 \t ep_len: 200.000\n",
      "epoch: 246 \t loss: 704.049 \t return: -1609.083 \t ep_len: 200.000\n",
      "epoch: 247 \t loss: 753.051 \t return: -1613.858 \t ep_len: 200.000\n",
      "epoch: 248 \t loss: 737.631 \t return: -1599.006 \t ep_len: 200.000\n",
      "epoch: 249 \t loss: 725.817 \t return: -1619.925 \t ep_len: 200.000\n",
      "epoch: 250 \t loss: 759.316 \t return: -1639.273 \t ep_len: 200.000\n",
      "epoch: 251 \t loss: 718.020 \t return: -1551.171 \t ep_len: 200.000\n",
      "epoch: 252 \t loss: 768.810 \t return: -1625.880 \t ep_len: 200.000\n",
      "epoch: 253 \t loss: 758.318 \t return: -1546.505 \t ep_len: 200.000\n",
      "epoch: 254 \t loss: 755.013 \t return: -1606.648 \t ep_len: 200.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 255 \t loss: 715.229 \t return: -1569.614 \t ep_len: 200.000\n",
      "epoch: 256 \t loss: 749.753 \t return: -1631.697 \t ep_len: 200.000\n",
      "epoch: 257 \t loss: 732.895 \t return: -1585.496 \t ep_len: 200.000\n",
      "epoch: 258 \t loss: 717.158 \t return: -1550.646 \t ep_len: 200.000\n",
      "epoch: 259 \t loss: 735.212 \t return: -1643.889 \t ep_len: 200.000\n",
      "epoch: 260 \t loss: 790.661 \t return: -1643.904 \t ep_len: 200.000\n",
      "epoch: 261 \t loss: 756.890 \t return: -1589.211 \t ep_len: 200.000\n",
      "epoch: 262 \t loss: 753.881 \t return: -1620.524 \t ep_len: 200.000\n",
      "epoch: 263 \t loss: 753.553 \t return: -1614.737 \t ep_len: 200.000\n",
      "epoch: 264 \t loss: 750.758 \t return: -1648.153 \t ep_len: 200.000\n",
      "epoch: 265 \t loss: 682.165 \t return: -1586.430 \t ep_len: 200.000\n",
      "epoch: 266 \t loss: 704.101 \t return: -1585.534 \t ep_len: 200.000\n",
      "epoch: 267 \t loss: 782.972 \t return: -1606.782 \t ep_len: 200.000\n",
      "epoch: 268 \t loss: 783.081 \t return: -1604.844 \t ep_len: 200.000\n",
      "epoch: 269 \t loss: 709.406 \t return: -1601.346 \t ep_len: 200.000\n",
      "epoch: 270 \t loss: 751.878 \t return: -1551.627 \t ep_len: 200.000\n",
      "epoch: 271 \t loss: 771.083 \t return: -1602.128 \t ep_len: 200.000\n",
      "epoch: 272 \t loss: 714.970 \t return: -1608.500 \t ep_len: 200.000\n",
      "epoch: 273 \t loss: 733.707 \t return: -1623.756 \t ep_len: 200.000\n",
      "epoch: 274 \t loss: 655.859 \t return: -1536.085 \t ep_len: 200.000\n",
      "epoch: 275 \t loss: 680.752 \t return: -1583.313 \t ep_len: 200.000\n",
      "epoch: 276 \t loss: 757.819 \t return: -1630.727 \t ep_len: 200.000\n",
      "epoch: 277 \t loss: 746.242 \t return: -1600.613 \t ep_len: 200.000\n",
      "epoch: 278 \t loss: 772.597 \t return: -1600.105 \t ep_len: 200.000\n",
      "epoch: 279 \t loss: 716.644 \t return: -1604.664 \t ep_len: 200.000\n",
      "epoch: 280 \t loss: 727.248 \t return: -1582.175 \t ep_len: 200.000\n",
      "epoch: 281 \t loss: 739.730 \t return: -1572.840 \t ep_len: 200.000\n",
      "epoch: 282 \t loss: 741.814 \t return: -1632.665 \t ep_len: 200.000\n",
      "epoch: 283 \t loss: 765.784 \t return: -1623.719 \t ep_len: 200.000\n",
      "epoch: 284 \t loss: 752.713 \t return: -1597.262 \t ep_len: 200.000\n",
      "epoch: 285 \t loss: 805.460 \t return: -1649.617 \t ep_len: 200.000\n",
      "epoch: 286 \t loss: 732.002 \t return: -1555.066 \t ep_len: 200.000\n",
      "epoch: 287 \t loss: 747.308 \t return: -1624.414 \t ep_len: 200.000\n",
      "epoch: 288 \t loss: 754.322 \t return: -1650.772 \t ep_len: 200.000\n",
      "epoch: 289 \t loss: 740.673 \t return: -1605.868 \t ep_len: 200.000\n",
      "epoch: 290 \t loss: 711.908 \t return: -1581.623 \t ep_len: 200.000\n",
      "epoch: 291 \t loss: 756.005 \t return: -1587.532 \t ep_len: 200.000\n",
      "epoch: 292 \t loss: 722.712 \t return: -1609.052 \t ep_len: 200.000\n",
      "epoch: 293 \t loss: 720.908 \t return: -1619.078 \t ep_len: 200.000\n",
      "epoch: 294 \t loss: 749.189 \t return: -1626.654 \t ep_len: 200.000\n",
      "epoch: 295 \t loss: 756.982 \t return: -1563.018 \t ep_len: 200.000\n",
      "epoch: 296 \t loss: 746.646 \t return: -1594.546 \t ep_len: 200.000\n",
      "epoch: 297 \t loss: 738.883 \t return: -1606.602 \t ep_len: 200.000\n",
      "epoch: 298 \t loss: 718.574 \t return: -1595.310 \t ep_len: 200.000\n",
      "epoch: 299 \t loss: 753.676 \t return: -1622.732 \t ep_len: 200.000\n",
      "epoch: 300 \t loss: 766.359 \t return: -1622.286 \t ep_len: 200.000\n",
      "epoch: 301 \t loss: 753.461 \t return: -1613.352 \t ep_len: 200.000\n",
      "epoch: 302 \t loss: 766.927 \t return: -1624.707 \t ep_len: 200.000\n",
      "epoch: 303 \t loss: 723.085 \t return: -1568.079 \t ep_len: 200.000\n",
      "epoch: 304 \t loss: 702.888 \t return: -1564.689 \t ep_len: 200.000\n",
      "epoch: 305 \t loss: 714.141 \t return: -1575.745 \t ep_len: 200.000\n",
      "epoch: 306 \t loss: 734.605 \t return: -1557.758 \t ep_len: 200.000\n",
      "epoch: 307 \t loss: 750.825 \t return: -1626.045 \t ep_len: 200.000\n",
      "epoch: 308 \t loss: 732.351 \t return: -1614.176 \t ep_len: 200.000\n",
      "epoch: 309 \t loss: 760.686 \t return: -1638.943 \t ep_len: 200.000\n",
      "epoch: 310 \t loss: 726.230 \t return: -1596.493 \t ep_len: 200.000\n",
      "epoch: 311 \t loss: 736.585 \t return: -1616.553 \t ep_len: 200.000\n",
      "epoch: 312 \t loss: 713.291 \t return: -1600.311 \t ep_len: 200.000\n",
      "epoch: 313 \t loss: 725.726 \t return: -1632.428 \t ep_len: 200.000\n",
      "epoch: 314 \t loss: 717.311 \t return: -1570.685 \t ep_len: 200.000\n",
      "epoch: 315 \t loss: 750.222 \t return: -1624.513 \t ep_len: 200.000\n",
      "epoch: 316 \t loss: 705.096 \t return: -1564.235 \t ep_len: 200.000\n",
      "epoch: 317 \t loss: 730.966 \t return: -1630.749 \t ep_len: 200.000\n",
      "epoch: 318 \t loss: 732.410 \t return: -1607.599 \t ep_len: 200.000\n",
      "epoch: 319 \t loss: 699.943 \t return: -1603.779 \t ep_len: 200.000\n",
      "epoch: 320 \t loss: 744.868 \t return: -1613.277 \t ep_len: 200.000\n",
      "epoch: 321 \t loss: 745.066 \t return: -1588.914 \t ep_len: 200.000\n",
      "epoch: 322 \t loss: 720.056 \t return: -1610.083 \t ep_len: 200.000\n",
      "epoch: 323 \t loss: 689.568 \t return: -1621.756 \t ep_len: 200.000\n",
      "epoch: 324 \t loss: 754.368 \t return: -1573.257 \t ep_len: 200.000\n",
      "epoch: 325 \t loss: 765.281 \t return: -1575.259 \t ep_len: 200.000\n",
      "epoch: 326 \t loss: 794.935 \t return: -1636.054 \t ep_len: 200.000\n",
      "epoch: 327 \t loss: 724.294 \t return: -1586.617 \t ep_len: 200.000\n",
      "epoch: 328 \t loss: 714.646 \t return: -1601.169 \t ep_len: 200.000\n",
      "epoch: 329 \t loss: 730.626 \t return: -1619.563 \t ep_len: 200.000\n",
      "epoch: 330 \t loss: 681.426 \t return: -1563.704 \t ep_len: 200.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-8c1053bb6508>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mrender\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#     render = False\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mbatch_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_ret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mmean_rew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mmean_rew_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmean_rew\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-540416e1d73e>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(env, net, lr, batch_size, render)\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mbatch_obs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mbatch_acts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-6fdf04cb6d4d>\u001b[0m in \u001b[0;36mget_action\u001b[1;34m(net, obs)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m#     if act<0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#         print (\"yes negative\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\gym\\lib\\site-packages\\torch\\distributions\\normal.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, sample_shape)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extended_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrsample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "rew_req = 200\n",
    "i=0\n",
    "mean_rew = 0\n",
    "mean_rew_sum = 0\n",
    "avg_rew=0\n",
    "\n",
    "while i<500:\n",
    "    i+=1\n",
    "    render = True if i%50==0 else False\n",
    "#     render = False\n",
    "    batch_loss,batch_ret, batch_len, obs, acts = train_one_epoch(env, net,batch_size=BATCH_SIZE, render=render)\n",
    "    mean_rew = np.mean(batch_ret)\n",
    "    mean_rew_sum += mean_rew\n",
    "    avg_rew = mean_rew_sum/i\n",
    "    \n",
    "    if render:\n",
    "        env.close()\n",
    "    print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "                (i, batch_loss, np.mean(batch_ret), np.mean(batch_len)))\n",
    "    writer.add_scalar(\"loss\", batch_loss, i)\n",
    "    writer.add_scalar(\"reward_mean\", mean_rew, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScratchPad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Normal(loc = torch.FloatTensor([1.0]), scale = torch.Tensor([1.0]))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7945])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a0d60ee6a1d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mcount\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not float"
     ]
    }
   ],
   "source": [
    "count = [0,0,0]\n",
    "for i in range(0,100):\n",
    "    x = c.sample().item()\n",
    "    count[x]+=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0071])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.log_prob(torch.Tensor(np.array([0.58])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2, 2, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x =([2]*8)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 2, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = ([1]*3 + [2]*5)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): data must be a sequence (got float)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-253457ef7542>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: new(): data must be a sequence (got float)"
     ]
    }
   ],
   "source": [
    "(torch.FloatTensor(x)*torch.FloatTensor(y)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=0\n",
    "for x in acts:\n",
    "    if x<0:\n",
    "        c+=1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 15.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 20.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 42.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 8.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 29.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 11.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 44.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 55.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 9.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 28.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 14.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 16.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 23.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 18.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 17.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 13.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 19.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 57.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 32.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 21.0,\n",
       " 15.0,\n",
       " ...]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3]]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3]\n",
    "y = []\n",
    "y.append(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3]]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[4,2,3]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.append(x.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 2, 3]]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 2, 3], [4, 2, 3]]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.3292841e-01,  1.3936392e+38,  2.2844117e-02,  8.5078421e+37],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.observation_space.sample()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([79045296309277814133001641918405279744.,\n",
       "         8074543887068950992878338433657864192.], grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.FloatTensor(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2 = Categorical(net(torch.FloatTensor(obs)))\n",
    "c2.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.3786, grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2.log_prob(torch.as_tensor(1, dtype = torch.int32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
